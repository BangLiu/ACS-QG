import os
import torch
import numpy as np
from tqdm import tqdm
from datetime import datetime
from .config import *
from data_loader.FQG_data import get_raw_examples, normalize_text
from data_augmentor import FQG_data_augmentor
from common.constants import NLP
from util.prepro_utils import get_token_char_level_spans
from data_loader.FQG_data_utils import get_question_type
from util.file_utils import load


def get_processed_examples(raw_examples, debug=False, debug_length=20, shuffle=False):
    """
    Get a list of spaCy processed examples given raw examples.
    """
    print("Start transform raw examples to processed examples...")
    start = datetime.now()
    examples = []
    meta = {}
    meta["num_q"] = 0
    num_spans_len_error = 0
    num_not_match_error = 0
    for e in tqdm(raw_examples):
        # paragraph info (here is sentence)
        ans_sent = normalize_text(e["ans_sent"])
        ans_sent_doc = NLP(ans_sent)
        ans_sent_tokens = [token.text for token in ans_sent_doc]
        spans = get_token_char_level_spans(ans_sent, ans_sent_tokens)

        # question info
        ques = normalize_text(e["question"])
        # ques = "<sos> " + ques + " <eos>"  # notice: this is important for QG
        ques_doc = NLP(ques)
        ques_type, ques_type_id = get_question_type(e["question"])

        # answer info
        answer_text = normalize_text(e["answer_text"])
        answer_start = e["answer_start"]
        answer_end = answer_start + len(answer_text)
        answer_span = []

        for idx, span in enumerate(spans):
            if not (answer_end <= span[0] or
                    answer_start >= span[1]):
                answer_span.append(idx)
        y1_in_sent, y2_in_sent = answer_span[0], answer_span[-1]
        answer_in_sent = " ".join(ans_sent_tokens[y1_in_sent:y2_in_sent + 1])

        # clue info
        clue_info = FQG_data_augmentor.get_clue_info(
            ques, ans_sent, answer_in_sent, None,
            chunklist=None, y1_in_sent=y1_in_sent,
            doc=ans_sent_doc, ques_doc=ques_doc, sent_limit=100)
        ans_sent_is_clue = clue_info["selected_clue_binary_ids_padded"]
        clue_token_position = np.where(ans_sent_is_clue == 1)[0]
        if clue_info["clue_chunk"] is not None:
            clue_tokenized_text = " ".join(clue_info["clue_chunk"][2])
        else:
            clue_tokenized_text = None
        if len(clue_token_position) > 0 and clue_tokenized_text is not None:
            clue_start_token_idx = clue_token_position[0]
            clue_end_token_idx = clue_token_position[-1]

            if len(spans) > clue_end_token_idx:  # !!! NOTICE: sometimes we have len(spans) <= clue_end_token_idx. Need further debug...
                clue_start = spans[clue_start_token_idx][0]
                clue_end = spans[clue_end_token_idx][1]  # [)
                clue_text = ans_sent[clue_start:clue_end]
            else:
                num_spans_len_error += 1
                clue_text = None

            if clue_text != clue_tokenized_text:
                if clue_text is not None:
                    num_not_match_error += 1
                clue_start = ans_sent.find(clue_tokenized_text)
                clue_end = clue_start + len(clue_tokenized_text)
                if clue_start > 0:  # not -1
                    clue_text = clue_tokenized_text
                    # print("clue_text revised: ", clue_text)
                    # print("clue_start revised: ", clue_start)
                else:
                    continue
        else:
            clue_text = None
            clue_start = None

        # final example
        example = {
            "paragraph": ans_sent,

            "question": ques,
            "ques_type": ques_type,  # string type

            "answer": answer_text,
            "answer_start": answer_start,

            "clue": clue_text,
            "clue_start": clue_start,
            "para_id": meta["num_q"]}
        examples.append(example)

        meta["num_q"] += 1

        if debug and meta["num_q"] >= debug_length:
            break

    if shuffle:
        random.shuffle(examples)

    print("num_not_match_error: ", num_not_match_error)
    print("num_spans_len_error: ", num_spans_len_error)
    print(("Time of get processed examples: {}").format(
        datetime.now() - start))
    print("Number of processed examples: ", len(examples))
    return examples


def get_augmented_sents_examples(augmented_sentences_pkl_file, debug=False, debug_length=20, sent_limit=100, ans_limit=30):
    """
    This is used to load the augmented sentences data that generated by DA_main.py
    """
    examples = load(augmented_sentences_pkl_file)
    result = []

    para_id = 0
    for example in tqdm(examples):
        ans_sent = example["context"]

        for info in example["selected_infos"]:
            answer_text = info["answer"]["answer_text"]
            answer_start = info["answer"]["char_start"]
            # filter
            answer_bio_ids = info["answer"]["answer_bio_ids"]
            answer_length = answer_bio_ids.count("B") + answer_bio_ids.count("I")
            if (len(example["ans_sent_doc"]) > sent_limit or answer_length > ans_limit):
                continue
            for clue in info["clues"]:
                clue_text = clue["clue_text"]
                clue_start = ans_sent.find(clue_text)
                if clue_start < 0:  # not -1
                    continue

                for style_text in info["styles"]:
                    output_e = {
                        "paragraph": ans_sent,

                        "question": "",
                        "ques_type": style_text,  # string type

                        "answer": answer_text,
                        "answer_start": answer_start,

                        "clue": clue_text,
                        "clue_start": clue_start,
                        "para_id": example["sid"]}  # because our paragraph is sentence actually.
                    result.append(output_e)
        para_id += 1
        if debug and para_id >= debug_length:
            break
    return result


def get_dataset(tokenizer, dataset_cache, path, split='train', filetype='squad', debug=False, debug_length=20):
    # Load question data
    if dataset_cache and os.path.isfile(dataset_cache):  #!!! NOTICE: make sure dataset_cache is correct version.
        print("Load tokenized dataset from cache at %s", dataset_cache)
        data = torch.load(dataset_cache)
        return data

    data = get_positional_dataset_from_file(tokenizer, file=path, filetype=filetype, debug=debug, debug_length=debug_length)

    if dataset_cache:
        torch.save(data, dataset_cache)

    print("Dataset cached at %s", dataset_cache)

    return data


def get_position(para_ids, ans_ids, ans_prefix_ids):
    diff_index = -1
    # Find the first token where the paragraph and answer prefix differ
    for i, (pid, apid) in enumerate(zip(para_ids, ans_prefix_ids)):
        if pid != apid:
            diff_index = i
            break
    if diff_index == -1:
        diff_index = min(len(ans_prefix_ids), len(para_ids))
    # Starting from this token, we take a conservative overlap
    return (diff_index, min(diff_index + len(ans_ids), len(para_ids)))


def get_positional_dataset_from_file(tokenizer, file, filetype="squad", debug=False, debug_length=20):
    if filetype == "augmented_sents":
        data = get_augmented_sents_examples(file, debug, debug_length)
    else:  # "squad"
        data = get_raw_examples(file, filetype, debug, debug_length)  # NOTICE: add handler for data augmented input data.
        data = get_processed_examples(data, debug)
    truncated_sequences = 0
    for inst in tqdm(data):
        inst['answer_position'] = inst['answer_start']
        clue_exist = (inst['clue_start'] is not None)
        if clue_exist:
            inst['clue_position'] = inst['clue_start']

        tokenized_para = tokenizer.tokenize(inst['paragraph'])
        tokenized_question = tokenizer.tokenize(inst['question'])
        tokenized_answer = tokenizer.tokenize(inst['answer'])
        tokenized_ans_prefix = tokenizer.tokenize(inst['paragraph'][:inst['answer_position']])

        if clue_exist:
            tokenized_clue = tokenizer.tokenize(inst['clue'])
            tokenized_clue_prefix = tokenizer.tokenize(inst['paragraph'][:inst['clue_position']])
        else:
            tokenized_clue = []

        tokenized_qtype = tokenizer.tokenize(inst['ques_type'])

        total_seq_len = len(tokenized_para) + len(tokenized_answer) + len(tokenized_question) + len(tokenized_clue) + len(tokenized_qtype) + 6

        if total_seq_len > tokenizer.max_len:
            # Heuristic to chop off extra tokens in paragraphs
            tokenized_para = tokenized_para[:-1 * (total_seq_len - tokenizer.max_len + 1)]
            truncated_sequences += 1
            assert len(tokenized_para) + len(tokenized_answer) + len(tokenized_question) + len(tokenized_clue) + len(tokenized_qtype) + 6 < tokenizer.max_len

        inst['paragraph'] = tokenizer.convert_tokens_to_ids(tokenized_para)
        inst['question'] = tokenizer.convert_tokens_to_ids(tokenized_question)
        inst['answer'] = tokenizer.convert_tokens_to_ids(tokenized_answer)
        ans_prefix_ids = tokenizer.convert_tokens_to_ids(tokenized_ans_prefix)
        inst['answer_position_tokenized'] = get_position(inst['paragraph'], inst['answer'], ans_prefix_ids)

        if clue_exist:
            inst['clue'] = tokenizer.convert_tokens_to_ids(tokenized_clue)
            clue_prefix_ids = tokenizer.convert_tokens_to_ids(tokenized_clue_prefix)
            inst['clue_position_tokenized'] = get_position(inst['paragraph'], inst['clue'], clue_prefix_ids)

        inst['style'] = tokenizer.convert_tokens_to_ids(tokenized_qtype)
        pass

    print("%d / %d sequences truncated due to positional embedding restriction" % (truncated_sequences, len(data)))

    return data


# examples = get_raw_examples("../../../../../Datasets/original/SQuAD1.1-Zhou/train.txt", filetype="squad", debug=True, debug_length=20)
# examples = get_processed_examples(examples, debug=True)
