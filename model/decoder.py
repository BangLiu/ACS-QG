"""
Decoder for text generation.
"""
import torch
import torch.nn as nn
from modules.maxout import MaxOut
from modules.rnn import StackedGRU
from modules.attention import ConcatAttention


class Decoder(nn.Module):
    """
    Decode with attention and copy mechanism.
    """

    def __init__(self, config, dec_input_size, dropout=0.1):
        """
        Initialize decoder RNN, attention, copy gate, etc.
        """
        super(Decoder, self).__init__()
        self.config = config
        self.layers = config.layers
        self.input_feed = config.input_feed
        input_size = dec_input_size  # in our model, it is the word embedding dimension
        if self.input_feed:  # whether concatenate the context vector with the word embeddings at each time step
            input_size += config.enc_rnn_size
        self.rnn = StackedGRU(  # NOTICE: why use StackedGRU instead of default multi-layer GRU?
            config.layers, input_size, config.dec_rnn_size, config.dropout)
        self.attn = ConcatAttention(
            config.enc_rnn_size, config.dec_rnn_size, config.att_vec_size)
        self.dropout = nn.Dropout(config.dropout)
        self.readout = nn.Linear(  # input context + decoder rnn output + previous word embedding, get an output vector for generator
            (config.enc_rnn_size + config.dec_rnn_size + dec_input_size),
            config.dec_rnn_size)
        self.maxout = MaxOut(config.maxout_pool_size)  # applied to readout output. it will reduce the hidden size by maxout.
        self.maxout_pool_size = config.maxout_pool_size
        if self.config.copy_type in ["soft", "soft-oov"]:
            self.copySwitch = nn.Sequential(
                nn.Linear(config.enc_rnn_size + config.dec_rnn_size, 3),  # NOTICE: 3: [not-copy, hard-copy, soft-copy]
                nn.Softmax(dim=1))
        else:
            self.copySwitch = nn.Linear(  # input context + decoder rnn output, decide the prob for copy.
                config.enc_rnn_size + config.dec_rnn_size, 1)
        self.hidden_size = config.dec_rnn_size

    def forward(self, output_emb, hidden, context, src_pad_mask, init_attn_weighted_context):
        """
        Generate decoder outputs with attention and copy given the following inputs.
        :param output_emb: the word embeddings of gold output tokens.
            Shape - [maximum_batch_seq_len, batch_size, emb_dim] [22, 32, 300]
        :param hidden: the hidden state of decoder. Initially, we use the initial
            hidden vector generated by decIniter (e.g., the last hidden state of encoder).
            Shape - [num_layers, batch_size, hidden_dim] [1, 32, 512]
        :param context:  the context representation of input sequence generated by encoder.
            It contains all the encoder layers context representation, and it will be used
            in attention mechanism to calculate a weighted sum representation of input.
            Shape - [max_batch_seq_len, batch_size, hidden_dim][79, 32, 512]
        :param src_pad_mask: 0 1 mask to indicate which part of input tokens are not pad.
            Shape - [batch_size, max_batch_seq_len][32, 79]
        :param init_attn_weighted_context: initial attention weighted input context representation.
            Shape - [batch_size, hidden_dim] [32, 512]
        :return:
            g_outputs - it will input to generator.
                Shape [max_batch_output_seq_len, batch_size, hidden_dim / max_out_size]
            c_outputs - scores over input tokens for copy.
                Shape [max_batch_output_seq_len, batch_size, max_batch_input_seq_len]
            copy_gate_outputs - copy gate values for each output token position.
                Shape [max_batch_output_seq_len, batch_size, 1] or 3] for soft copy.
            hidden - decoder rnn hidden state.
                Shape [num_layers, batch_size, hidden_dim]
            attn - the attn scores over input tokens in the last time step.
                Shape [batch_size, max_batch_input_seq_len]
            current_attn_weighted_context - the weighted input context vector representation.
                Shape [batch_size, hidden_dim]
        """
        g_outputs = []
        c_outputs = []
        copy_gate_outputs = []
        current_attn_weighted_context = init_attn_weighted_context
        self.attn.applyMask(src_pad_mask)
        precompute = None

        # iterate over each output step (each token)
        for emb_t in output_emb.split(1):  # dim=0, split size=1
            # previous output word embedding.
            # tgt start with <sos>, so we can use t rather than t - 1.
            emb_t = emb_t.squeeze(0)  # emb_t shape: [batch_size, emb_dim] [32, 300]
            decoder_rnn_input_t = emb_t
            # concatenate previous word embedding with previous
            # attention context, use them as decoder rnn input
            if self.input_feed:
                decoder_rnn_input_t = torch.cat([emb_t, current_attn_weighted_context], 1)

            # get output and new hidden state by decoder rnn
            output, hidden = self.rnn(decoder_rnn_input_t, hidden)  # !!!!!!!!
            # decoder rnn output shape: [batch_size, hidden_dim] [32, 512]
            # decoder rnn hidden shape: [num_layers, batch_size, hidden_dim] [1, 32, 512]

            # update new attention context
            current_attn_weighted_context, attn, precompute = self.attn(
                output, context.transpose(0, 1), precompute)
            # current_attn_weighted_context.shape: [batch_size, encode_dim]
            # attn.shape: [batch_size, max_batch_input_seq_len]
            # precompute.shape: [batch_size, max_batch_input_seq_len, encode_dim]

            # use current decoder output + attention context to get copy prob
            if self.config.copy_type in ["soft", "soft-oov"]:
                copyProb = self.copySwitch(torch.cat((output, current_attn_weighted_context), dim=1))
            else:
                copyProb = self.copySwitch(torch.cat((output, current_attn_weighted_context), dim=1))
                copyProb = torch.sigmoid(copyProb)

            # use previous word emb + current rnn output +
            # current attention context, to get final generate output vector
            # which will be fed into generator to generate current output
            # word probabilities
            readout = self.readout(torch.cat((emb_t, output, current_attn_weighted_context), dim=1))  # [batch_size, hidden_dim] [32, 512]
            output = self.dropout(self.maxout(readout))  # [batch_size, hidden_dim / max_out_size] [32, 256]

            g_outputs += [output]
            c_outputs += [attn]  # use the attention between current rnn output and context as copy prob of input words
            copy_gate_outputs += [copyProb]
        g_outputs = torch.stack(g_outputs)  # [max_batch_output_seq_len, batch_size, hidden_dim / max_out_size] [22, 32, 256]
        c_outputs = torch.stack(c_outputs)  # [max_batch_output_seq_len, batch_size, max_batch_input_seq_len] [22, 32, 79]
        copy_gate_outputs = torch.stack(copy_gate_outputs)  # [max_batch_output_seq_len, batch_size, 1] [22, 32, 1]
        return g_outputs, c_outputs, copy_gate_outputs, hidden, attn, current_attn_weighted_context
